{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9efInp3Wlup",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf python-dotenv transformers llama-index llama-index-postprocessor-colbert-rerank llama-index-embeddings-huggingface llama-index-embeddings-instructor sentence-transformers llama-index-vector-stores-chroma chromadb flask flask_ngrok pyngrok flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWLstQLPCjk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21015fc3-4e11-4211-9d04-b03f311bd5e7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp310-cp310-linux_x86_64.whl size=169125856 sha256=0c479fe3869f76e62d5b8d68b6850d685898a001805b7b49db122af791fdbcad\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7zewhgud/wheels/fd/c5/bd/3b1c20081bd71ce9d28b562573c97915c790bf1ef231879a61\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install  llama-cpp-python --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkHavErBXChp"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader,ServiceContext,Settings\n",
        "API_KEY=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHde_K6n2zVG",
        "outputId": "0539ec3a-66b9-403b-eaee-1e9c9b2d0e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXk1KTc6C3Lc"
      },
      "outputs": [],
      "source": [
        "!cp -r ./drive/MyDrive/chemistri ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COxp34FqXMih"
      },
      "outputs": [],
      "source": [
        "# documents = SimpleDirectoryReader(\"/content/Data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zRgNPiM3zt3",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c9a97d-f086-43c9-bc16-7bb62b3c5608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-llama-cpp\n",
            "  Downloading llama_index_llms_llama_cpp-0.1.3-py3-none-any.whl (5.1 kB)\n",
            "Collecting llama-index-tools-tavily-research\n",
            "  Downloading llama_index_tools_tavily_research-0.1.3-py3-none-any.whl (3.3 kB)\n",
            "Requirement already satisfied: llama-cpp-python<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-llama-cpp) (0.2.78)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-llama-cpp) (0.10.44)\n",
            "Collecting tavily-python>=0.2.4 (from llama-index-tools-tavily-research)\n",
            "  Downloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (3.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.34.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (4.66.4)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (2.1.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.7.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.16.0)\n",
            "Installing collected packages: tavily-python, llama-index-tools-tavily-research, llama-index-llms-llama-cpp\n",
            "Successfully installed llama-index-llms-llama-cpp-0.1.3 llama-index-tools-tavily-research-0.1.3 tavily-python-0.3.3\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-llama-cpp llama-index-tools-tavily-research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BhyzesSOYuRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10abdd8-ff52-4032-a533-7ac2efafb57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading url https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf to path /tmp/llama_index/models/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
            "total size (MB): 7695.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▎     | 3199/7339 [06:17<08:46,  7.86it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf',\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=4000,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    # transform inputs into Llama2 format\n",
        "    # messages_to_prompt=messages_to_prompt,\n",
        "    # completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")\n",
        "Settings.llm=llm\n",
        "Settings.chunk_size=512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mT0mMqtctGl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeTUYneS501Q"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import chromadb\n",
        "from chromadb.config import Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwGjWnoNai9y"
      },
      "outputs": [],
      "source": [
        "# index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RECcmxuu5QSL"
      },
      "outputs": [],
      "source": [
        "db = chromadb.PersistentClient(path=\"./chemistri\")\n",
        "\n",
        "# get collection\n",
        "chroma_collection = db.get_or_create_collection(\"documents\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# load your index from stored vectors\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store, storage_context=storage_context,embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdvCSPrm6Omj"
      },
      "outputs": [],
      "source": [
        "# db = chromadb.PersistentClient(path=\"./vectors\",settings=Settings(anonymized_telemetry=False))\n",
        "\n",
        "# # get collection\n",
        "# chroma_collection = db.get_or_create_collection(\"documents\")\n",
        "\n",
        "# # assign chroma as the vector_store to the context\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# # load your index from stored vectors\n",
        "# index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store, storage_context=storage_context,embed_model=embed_model\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iHx1ZGWYrYh"
      },
      "outputs": [],
      "source": [
        "ret = index.as_retriever()\n",
        "print([n.text[0:300] for n in ret.retrieve(\"what is physical chemistry?\")])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_lY8NE5aM_q"
      },
      "outputs": [],
      "source": [
        "# from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
        "# # from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "# colbert_reranker = ColbertRerank(\n",
        "#     top_n=3,\n",
        "#     model=\"colbert-ir/colbertv2.0\",\n",
        "#     tokenizer=\"colbert-ir/colbertv2.0\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1haheB1ndQ1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt_tmpl_str=\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge,\n",
        "respond to the user query below only if it's related to chemistry. If the query is not related to chemistry, respond with \"I only answer questions related to chemistry.\" Do not add information from the context if it is not chemistry-related. If you don't know the answer to a chemistry question, simply respond with \"I don't know.\" Do not attempt to fabricate an answer.\n",
        "If context is empty or if there is no context present above then say \"I don't have information to answer the query.\" Don't try to make up an answer when the query is not chemistry-related.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# qa_prompt_tmpl_str=\"\"\"\n",
        "# Context information is below.\n",
        "# ---------------------\n",
        "# {context_str}\n",
        "# ---------------------\n",
        "# Given the context information and not prior knowledge,\n",
        "# respond to the user query below only if it's related to chemistry. If the query is not related to chemistry, respond with \"I only answer questions related to chemistry.\" Do not add information from the context if it is not chemistry-related. If you don't know the answer to a chemistry question, simply respond with \"I don't know.\" Do not attempt to fabricate an answer.\n",
        "# if context is empty then say I don't have information to answer the query. \\\n",
        "# don't try to make up an answer when query is not chemistry related. \\\n",
        "# Query: {query_str}\n",
        "# Answer:\n",
        "# \"\"\"\n",
        "\n",
        "# qa_prompt_tmpl_str = \"\"\"\\\n",
        "# Context information is below.\n",
        "# ---------------------\n",
        "# {context_str}\n",
        "# ---------------------\n",
        "# Given the context information and not prior knowledge, \\\n",
        "# Given the user query below check if it's related to chemistry subject if it's not then say \"I only answer questions related to chemistry\" and don't try to add from the context. \\\n",
        "# Answer the query asking about chemistry related topics only. \\\n",
        "# If you don't know the answer, just say you don't know, don't try to make up an answer. \\\n",
        "# Query: {query_str}\n",
        "# Answer: \\\n",
        "# \"\"\"\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(\n",
        "    qa_prompt_tmpl_str\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZztrkeB6S0X"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.llama_pack.base import BaseLlamaPack\n",
        "from llama_index.core.schema import Document, NodeWithScore\n",
        "from llama_index.core.query_pipeline.query import QueryPipeline\n",
        "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
        "    Retrieved document: \\n\\n {context_str}\n",
        "    \\n\\n User question: {query_str}\n",
        "    \\n\\n Evaluation('yes' or 'no'):\"\"\"\n",
        "DEFAULT_RELEVANCY_PROMPT_TEMPLATE = PromptTemplate(\n",
        "    template=system\n",
        ")\n",
        "# DEFAULT_RELEVANCY_PROMPT_TEMPLATE = PromptTemplate(\n",
        "# #     template1=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
        "# # \"\"\"\n",
        "# template=\"\"\"Task: Evaluate Document Relevance\n",
        "\n",
        "#     Retrieved Document:\n",
        "#     -------------------\n",
        "#     {context_str}\n",
        "\n",
        "#     User Question:\n",
        "#     --------------\n",
        "#     {query_str}\n",
        "\n",
        "#     Evaluation Criteria:\n",
        "#     -------------------\n",
        "#     Determine if the document contains keywords or topics related to the user's question.\n",
        "#     The goal is to identify and filter out clearly irrelevant documents.\n",
        "#     The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
        "\n",
        "#     Decision:\n",
        "#     ---------\n",
        "#     - reply with 'yes' if the document is relevant to the question.\n",
        "#     - reply with 'no' if the document is not relevant.\n",
        "#     - don't provide any explanation\n",
        "#     - you should only respond with 'yes' or 'no'.\n",
        "#     - document relevant (yes or no): \"\"\"\n",
        "# )\n",
        "\n",
        "DEFAULT_TRANSFORM_QUERY_TEMPLATE = PromptTemplate(\n",
        "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
        "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
        "    Original Query:\n",
        "    \\n ------- \\n\n",
        "    {query_str}\n",
        "    \\n ------- \\n\n",
        "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
        "    Respond with the optimized query only:\"\"\"\n",
        ")\n",
        "from llama_index.core import get_response_synthesizer\n",
        "\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"refine\")\n",
        "\n",
        "class CorrectiveRAG(BaseLlamaPack):\n",
        "    def __init__(self, index, tavily_ai_apikey: str,llm) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "        self.llm = llm\n",
        "        self.relevancy_pipeline = QueryPipeline(\n",
        "            chain=[DEFAULT_RELEVANCY_PROMPT_TEMPLATE, llm]\n",
        "        )\n",
        "        self.transform_query_pipeline = QueryPipeline(\n",
        "            chain=[DEFAULT_TRANSFORM_QUERY_TEMPLATE, llm]\n",
        "        )\n",
        "        self.index = index\n",
        "        self.tavily_tool = TavilyToolSpec(api_key=tavily_ai_apikey)\n",
        "\n",
        "    def get_modules(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get modules.\"\"\"\n",
        "        return {\"llm\": self.llm, \"index\": self.index}\n",
        "\n",
        "    def retrieve_nodes(self, query_str: str, **kwargs: Any) -> List[NodeWithScore]:\n",
        "        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
        "        retriever = self.index.as_retriever(**kwargs)\n",
        "        a = retriever.retrieve(query_str)\n",
        "        return a\n",
        "\n",
        "    def evaluate_relevancy(\n",
        "        self, retrieved_nodes: List[Document], query_str: str\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Evaluate relevancy of retrieved documents with the query.\"\"\"\n",
        "        relevancy_results = []\n",
        "        for node in retrieved_nodes:\n",
        "            relevancy = self.relevancy_pipeline.run(\n",
        "                context_str=node.text, query_str=query_str\n",
        "            )\n",
        "            relevancy_results.append(relevancy.text)\n",
        "        return relevancy_results\n",
        "\n",
        "    def extract_relevant_texts(\n",
        "        self, retrieved_nodes: List[NodeWithScore], relevancy_results: List[str]\n",
        "    ) -> str:\n",
        "        \"\"\"Extract relevant texts from retrieved documents.\"\"\"\n",
        "        relevant_texts = [\n",
        "            retrieved_nodes[i].text\n",
        "            for i, result in enumerate(relevancy_results)\n",
        "            if \"yes\" in result or \"Yes\" in result or \"YES\" in result\n",
        "        ]\n",
        "        return \"\\n\".join(relevant_texts)\n",
        "\n",
        "    def search_with_transformed_query(self, query_str: str) -> str:\n",
        "        \"\"\"Search the transformed query with Tavily API.\"\"\"\n",
        "        search_results = self.tavily_tool.search(query_str, max_results=5)\n",
        "        return \"\\n\".join([result.text for result in search_results])\n",
        "\n",
        "    def get_result(self, relevant_text: str, search_text: str, query_str: str) -> Any:\n",
        "        \"\"\"Get result with relevant text.\"\"\"\n",
        "        documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
        "        if len(documents[0].text) <=2:\n",
        "            return \"I don't have information to answer the query.\"\n",
        "        # index = VectorStoreIndex.from_documents(documents, optimize_for='speed')\n",
        "        index = SummaryIndex.from_documents(documents)\n",
        "        query_engine = index.as_query_engine(llm=llm,text_qa_template=qa_prompt_tmpl)\n",
        "        return query_engine.query(query_str)\n",
        "        # response_synthesizer=response_synthesizer,\n",
        "\n",
        "    def run(self, query_str: str, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run the pipeline.\"\"\"\n",
        "        # Retrieve nodes based on the input query string.\n",
        "        retrieved_nodes = self.retrieve_nodes(query_str, **kwargs)\n",
        "\n",
        "        # Evaluate the relevancy of each retrieved document in relation to the query string.\n",
        "        relevancy_results = self.evaluate_relevancy(retrieved_nodes, query_str)\n",
        "\n",
        "        # Extract texts from documents that are deemed relevant based on the evaluation.\n",
        "        relevant_text = self.extract_relevant_texts(retrieved_nodes, relevancy_results)\n",
        "\n",
        "        # Initialize search_text variable to handle cases where it might not get defined.\n",
        "        search_text = \"\"\n",
        "\n",
        "        # If any document is found irrelevant, transform the query string for better search results.\n",
        "        for c in relevancy_results:\n",
        "             if \"yes\" not in c.lower():\n",
        "                 print(\"no relevant in docs\")\n",
        "                 transformed_query_str = self.transform_query_pipeline.run(\n",
        "                     query_str=query_str\n",
        "                   ).text\n",
        "                 print(transformed_query_str)\n",
        "            # Conduct a search with the transformed query string and collect the results.\n",
        "                 search_text = self.search_with_transformed_query(transformed_query_str)\n",
        "                 break\n",
        "\n",
        "\n",
        "        # Compile the final result. If there's additional search text from the transformed query,\n",
        "        # it's included; otherwise, only the relevant text from the initial retrieval is returned.\n",
        "        if search_text:\n",
        "            return self.get_result(relevant_text, search_text, query_str)\n",
        "        else:\n",
        "            return self.get_result(relevant_text, \"\", query_str)\n",
        "crag = CorrectiveRAG(index,API_KEY,llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtMb8WENHcl9"
      },
      "outputs": [],
      "source": [
        "print(crag.run(\"Explain Hydrogen bond\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbikmS8ulJ91"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken 2hTSXbL3QBrRB2dIPNCWko4EnBa_fibWxGZzRHSapCcbRBtA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01WeKyzm8c7R"
      },
      "outputs": [],
      "source": [
        "# from pyngrok import ngrok\n",
        "\n",
        "# # [<NgrokTunnel: \"https://<public_sub>.ngrok.io\" -> \"http://localhost:80\">]\n",
        "# tunnels = ngrok.get_tunnels()\n",
        "# for t in tunnels:\n",
        "#     print(t)\n",
        "#     ngrok.disconnect(t.public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJvEx-5k1r9b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "from flask_cors import CORS\n",
        "from flask import Flask\n",
        "from flask import request,jsonify\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = 6248\n",
        "cors = CORS(app)\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"Hello World!\"\n",
        "@app.route(\"/query\", methods=[\"GET\"])\n",
        "def query_index():\n",
        "    query_text = request.args.get(\"text\", None)\n",
        "    if query_text is None:\n",
        "        return (\n",
        "            \"No text found, please include a ?text=blah parameter in the URL\",\n",
        "            400,\n",
        "        )\n",
        "    print(query_text)\n",
        "    resp = str(crag.run(query_text))\n",
        "    print(resp)\n",
        "    return jsonify({\"resp\":resp}),200\n",
        "\n",
        "# Start the Flask server in a new thread\n",
        "threading.Thread(target=app.run, kwargs={\"port\": port}).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fio3BKr2-vD8",
        "outputId": "4ae46c49-0859-4b7b-d36a-7c81c979950d",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is physcial keyboard\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     711.56 ms\n",
            "llama_print_timings:      sample time =      32.03 ms /    52 runs   (    0.62 ms per token,  1623.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     976.10 ms /   909 tokens (    1.07 ms per token,   931.26 tokens per second)\n",
            "llama_print_timings:        eval time =    1897.87 ms /    51 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
            "llama_print_timings:       total time =    2939.85 ms /   960 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     711.56 ms\n",
            "llama_print_timings:      sample time =      28.41 ms /    46 runs   (    0.62 ms per token,  1618.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     886.63 ms /   888 tokens (    1.00 ms per token,  1001.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1736.54 ms /    45 runs   (   38.59 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2685.54 ms /   933 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' no. The document does not contain any information about a physical keyboard. It appears to be the copyright and publication information for a textbook on Chemistry, Part II for Class XII published by the National Council of Educational Research and Training (NCERT).', ' no. The document does not contain any information about a physical keyboard. It appears to be an excerpt from the copyright page of a chemistry textbook published by NCERT (National Council of Educational Research and Training).']\n",
            "no relevant in docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     711.56 ms\n",
            "llama_print_timings:      sample time =       5.63 ms /    10 runs   (    0.56 ms per token,  1776.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     193.98 ms /   119 tokens (    1.63 ms per token,   613.48 tokens per second)\n",
            "llama_print_timings:        eval time =     350.09 ms /     9 runs   (   38.90 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =     555.10 ms /   128 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "    What is a physical keyboard?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     711.56 ms\n",
            "llama_print_timings:      sample time =      31.77 ms /    47 runs   (    0.68 ms per token,  1479.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     936.42 ms /   958 tokens (    0.98 ms per token,  1023.05 tokens per second)\n",
            "llama_print_timings:        eval time =    1715.19 ms /    46 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
            "llama_print_timings:       total time =    2721.66 ms /  1004 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Jun/2024 20:19:24] \"GET /query?text=what+is+physcial+keyboard HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I only answer questions related to chemistry.\n",
            "When query is about computer hardware, such as \"what is a physical keyboard\", I don't have information to answer the query. Don't try to make up an answer.\n",
            "127.0.0.1 - - [05/Jun/2024 20:19:24] \"GET /query?text=what+is+physcial+keyboard HTTP/1.1\" 200 -\n",
            "{\"resp\":\"I only answer questions related to chemistry.\\nWhen query is about computer hardware, such as \\\"what is a physical keyboard\\\", I don't have information to answer the query. Don't try to make up an answer.\"}\n"
          ]
        }
      ],
      "source": [
        "# # Use the public URL to make a curl request\n",
        "# !curl \"localhost:6247/query?text=what+is+physcial+keyboard\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgVwy4H6lAr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f59e3b5-d87f-4723-bd5f-b7921595fa83"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [13/Jun/2024 08:29:32] \"OPTIONS /query?text=What+are+different+bonds+in+chemistry? HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127.0.0.1 - - [13/Jun/2024 08:29:32] \"OPTIONS /query?text=What+are+different+bonds+in+chemistry? HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =      71.17 ms /   120 runs   (    0.59 ms per token,  1685.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =     220.78 ms /   266 tokens (    0.83 ms per token,  1204.79 tokens per second)\n",
            "llama_print_timings:        eval time =    4085.36 ms /   119 runs   (   34.33 ms per token,    29.13 tokens per second)\n",
            "llama_print_timings:       total time =    4448.21 ms /   385 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =      33.24 ms /    59 runs   (    0.56 ms per token,  1774.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.02 ms /   470 tokens (    0.63 ms per token,  1593.13 tokens per second)\n",
            "llama_print_timings:        eval time =    2000.17 ms /    58 runs   (   34.49 ms per token,    29.00 tokens per second)\n",
            "llama_print_timings:       total time =    2358.03 ms /   528 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no relevant in docs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =       4.47 ms /     8 runs   (    0.56 ms per token,  1788.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     154.46 ms /   119 tokens (    1.30 ms per token,   770.42 tokens per second)\n",
            "llama_print_timings:        eval time =     239.59 ms /     7 runs   (   34.23 ms per token,    29.22 tokens per second)\n",
            "llama_print_timings:       total time =     403.18 ms /   126 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "    Types of chemical bonds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =     210.52 ms /   346 runs   (    0.61 ms per token,  1643.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     821.21 ms /  1230 tokens (    0.67 ms per token,  1497.79 tokens per second)\n",
            "llama_print_timings:        eval time =   12092.29 ms /   345 runs   (   35.05 ms per token,    28.53 tokens per second)\n",
            "llama_print_timings:       total time =   13428.88 ms /  1575 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [13/Jun/2024 08:29:55] \"GET /query?text=What+are+different+bonds+in+chemistry? HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127.0.0.1 - - [13/Jun/2024 08:29:55] \"GET /query?text=What+are+different+bonds+in+chemistry? HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [13/Jun/2024 08:30:29] \"OPTIONS /query?text=What+is+the+IUPAC+name+of+C6H6+? HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127.0.0.1 - - [13/Jun/2024 08:30:29] \"OPTIONS /query?text=What+is+the+IUPAC+name+of+C6H6+? HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =      34.69 ms /    60 runs   (    0.58 ms per token,  1729.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =     582.49 ms /   904 tokens (    0.64 ms per token,  1551.96 tokens per second)\n",
            "llama_print_timings:        eval time =    2070.12 ms /    59 runs   (   35.09 ms per token,    28.50 tokens per second)\n",
            "llama_print_timings:       total time =    2725.98 ms /   963 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =      20.96 ms /    37 runs   (    0.57 ms per token,  1765.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     557.71 ms /   807 tokens (    0.69 ms per token,  1447.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1260.96 ms /    36 runs   (   35.03 ms per token,    28.55 tokens per second)\n",
            "llama_print_timings:       total time =    1859.41 ms /   843 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no relevant in docs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =       6.72 ms /    12 runs   (    0.56 ms per token,  1785.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     154.79 ms /   125 tokens (    1.24 ms per token,   807.52 tokens per second)\n",
            "llama_print_timings:        eval time =     377.25 ms /    11 runs   (   34.30 ms per token,    29.16 tokens per second)\n",
            "llama_print_timings:       total time =     544.58 ms /   136 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "    IUPAC name of benzene\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10731.43 ms\n",
            "llama_print_timings:      sample time =      38.24 ms /    67 runs   (    0.57 ms per token,  1752.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2499.37 ms /  3326 tokens (    0.75 ms per token,  1330.74 tokens per second)\n",
            "llama_print_timings:        eval time =    2414.66 ms /    66 runs   (   36.59 ms per token,    27.33 tokens per second)\n",
            "llama_print_timings:       total time =    5006.11 ms /  3392 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [13/Jun/2024 08:30:41] \"GET /query?text=What+is+the+IUPAC+name+of+C6H6+? HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127.0.0.1 - - [13/Jun/2024 08:30:41] \"GET /query?text=What+is+the+IUPAC+name+of+C6H6+? HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask\n",
        "from flask_cors import CORS\n",
        "from flask import request,stream_with_context,jsonify\n",
        "app = Flask(__name__)\n",
        "cors = CORS(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"Hello World!\"\n",
        "@app.route(\"/query\", methods=[\"GET\"])\n",
        "def query_index():\n",
        "    query_text = request.args.get(\"text\", None)\n",
        "    if query_text is None:\n",
        "        return (\n",
        "            \"No text found, please include a ?text=blah parameter in the URL\",\n",
        "            400,\n",
        "        )\n",
        "    return jsonify({\"resp\":str(crag.run(query_text))}),200\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUJRIfTWtS32"
      },
      "outputs": [],
      "source": [
        "!ngrok http 5000 #run this in terminal"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}